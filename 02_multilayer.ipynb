{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quartz\n",
    "from quartz import layers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "np.random.seed(seed=45)\n",
    "\n",
    "input_dims = (1,1,5,5,)\n",
    "l1_output_dim = 100\n",
    "l2_output_dim = 10\n",
    "\n",
    "weights1 = (np.random.rand(l1_output_dim, np.product(input_dims[1:])) - 0.5) / 2\n",
    "biases1 = (np.random.rand(l1_output_dim) - 0.5) / 2\n",
    "weights2 = (np.random.rand(l2_output_dim, l1_output_dim) - 0.5) / 2\n",
    "biases2 = (np.random.rand(l2_output_dim) - 0.5) / 2\n",
    "inputs = np.random.rand(*input_dims) / 2\n",
    "\n",
    "# note how the second layer does not have relu activation\n",
    "pt_model = nn.Sequential(\n",
    "    nn.Linear(in_features=np.product(input_dims[1:]), out_features=l1_output_dim), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=l1_output_dim, out_features=l2_output_dim),\n",
    ")\n",
    "pt_model[0].weight = nn.Parameter(torch.tensor(weights1))\n",
    "pt_model[0].bias = nn.Parameter(torch.tensor(biases1))\n",
    "pt_model[2].weight = nn.Parameter(torch.tensor(weights2))\n",
    "pt_model[2].bias = nn.Parameter(torch.tensor(biases2))\n",
    "pt_model(torch.tensor(inputs).reshape(input_dims[0],-1)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**8\n",
    "# we turn off relu activation in the SNN as well to get negative outputs\n",
    "loihi_model = quartz.Network(t_max, [\n",
    "    layers.InputLayer(dims=input_dims[1:]),\n",
    "    layers.Dense(weights=weights1, biases=biases1),\n",
    "    layers.Dense(weights=weights2, biases=biases2, rectifying=False),\n",
    "])\n",
    "\n",
    "loihi_model(inputs, logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quartz\n",
    "from quartz import layers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "np.random.seed(seed=45)\n",
    "\n",
    "input_dims = (1,1,10,10)\n",
    "conv_weight_dims1 = (6,1,8,8)\n",
    "conv_weight_dims2 = (5,6,3,3)\n",
    "\n",
    "conv_kernel_size1 = conv_weight_dims1[2:]\n",
    "conv_kernel_size2 = conv_weight_dims2[2:]\n",
    "\n",
    "weights1 = (np.random.rand(*conv_weight_dims1)-0.5) / 2\n",
    "biases1 = (np.random.rand(conv_weight_dims1[0])-0.5) / 2\n",
    "weights2 = (np.random.rand(*conv_weight_dims2)-0.5) / 2\n",
    "biases2 = (np.random.rand(conv_weight_dims2[0])-0.5) / 2\n",
    "inputs = np.random.rand(*input_dims) / 3\n",
    "\n",
    "pt_model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=conv_weight_dims1[1], out_channels=conv_weight_dims1[0], kernel_size=conv_kernel_size1), nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=conv_weight_dims2[1], out_channels=conv_weight_dims2[0], kernel_size=conv_kernel_size2), nn.ReLU(),\n",
    ")\n",
    "pt_model[0].weight = nn.Parameter(torch.tensor(weights1))\n",
    "pt_model[0].bias = nn.Parameter(torch.tensor(biases1))\n",
    "pt_model[2].weight = nn.Parameter(torch.tensor(weights2))\n",
    "pt_model[2].bias = nn.Parameter(torch.tensor(biases2))\n",
    "pt_model(torch.tensor(inputs)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**7\n",
    "loihi_model = quartz.Network(t_max, [\n",
    "    layers.InputLayer(dims=input_dims[1:]),\n",
    "    layers.Conv2D(weights=weights1, biases=biases1),\n",
    "    layers.Conv2D(weights=weights2, biases=biases2),\n",
    "])\n",
    "\n",
    "loihi_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quartz\n",
    "from quartz import layers\n",
    "import numpy as np\n",
    "\n",
    "input_dims = (1,1,8,8)\n",
    "t_max = 2**7\n",
    "kernel_size = [2,2]\n",
    "np.random.seed(seed=27)\n",
    "inputs = np.random.rand(*input_dims) / 2\n",
    "\n",
    "loihi_model = quartz.Network(t_max, [\n",
    "    layers.InputLayer(dims=input_dims[1:]),\n",
    "    layers.MaxPool2D(kernel_size=kernel_size),\n",
    "    layers.MaxPool2D(kernel_size=kernel_size),\n",
    "])\n",
    "loihi_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loihi_model.layers[2].rectifier_neurons[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-layer convolutional / maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quartz\n",
    "from quartz import layers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "np.random.seed(seed=44)\n",
    "\n",
    "input_dims = (10,1,10,10)\n",
    "weight_dims = ( 3,1,3,3)\n",
    "weight_dims2 = (5,3,3,3)\n",
    "pooling_kernel_size = [2,2]\n",
    "\n",
    "weights1 = (np.random.rand(*weight_dims)-0.5) / 3\n",
    "weights2 = (np.random.rand(*weight_dims2)-0.5) / 3\n",
    "biases1 = (np.random.rand(weight_dims[0])-0.5) / 3\n",
    "biases2 = (np.random.rand(weight_dims2[0])-0.5) / 3\n",
    "inputs = np.random.rand(*input_dims) / 3\n",
    "\n",
    "pt_model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=weight_dims[1], out_channels=weight_dims[0], kernel_size=weight_dims[2:]), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=pooling_kernel_size), nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=weight_dims2[1], out_channels=weight_dims2[0], kernel_size=weight_dims2[2:]), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=pooling_kernel_size), nn.ReLU(),\n",
    ")\n",
    "pt_model[0].weight = nn.Parameter(torch.tensor(weights1))\n",
    "pt_model[0].bias = nn.Parameter(torch.tensor(biases1))\n",
    "pt_model[4].weight = nn.Parameter(torch.tensor(weights2))\n",
    "pt_model[4].bias = nn.Parameter(torch.tensor(biases2))\n",
    "model_output = pt_model(torch.tensor(inputs)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**7\n",
    "loihi_model = quartz.Network(t_max, [\n",
    "    layers.InputLayer(dims=input_dims[1:]),\n",
    "    layers.Conv2D(weights=weights1, biases=biases1),\n",
    "    layers.MaxPool2D(kernel_size=pooling_kernel_size),\n",
    "    layers.Conv2D(weights=weights2, biases=biases2),\n",
    "    layers.MaxPool2D(kernel_size=pooling_kernel_size),\n",
    "])\n",
    "\n",
    "loihi_model(inputs, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nxsdk-1.0.0",
   "language": "python",
   "name": "nxsdk-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

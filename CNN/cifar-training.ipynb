{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ConvNet on CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import collections\n",
    "from functools import partial\n",
    "from cifar_model import MobileNet\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check device\n",
    "device = 'cuda'\n",
    "\n",
    "# parameters\n",
    "RANDOM_SEED = 42\n",
    "learning_rate = 0.001\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "num_workers = 10\n",
    "n_classes = 10\n",
    "activation_cutoff = 99.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(15, translate=(0.05,0.05)),\n",
    "    #transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# download and create datasets\n",
    "download = False\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=download)\n",
    "valid_dataset = datasets.CIFAR10(root='./data', train=False,transform=transform_test, download=download)\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size_test, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check augmented example image\n",
    "image, sample = next(iter(DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, num_workers=num_workers)))\n",
    "plt.imshow(image[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def save_activation(name, mod, inp, out):\n",
    "    activations[name] = out # don't detach or move to CPU here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        handles = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                handles.append(module.register_forward_hook(partial(save_activation, name)))\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device, activations)\n",
    "        train_losses.append(train_loss)\n",
    "        [handle.remove() for handle in handles] # remove forward hooks\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            train_acc = get_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device, activations):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, (X, y_true) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        # Forward pass\n",
    "        y_hat = model(X)\n",
    "        norm = 2\n",
    "        beta = 1e-4\n",
    "        loss = criterion(y_hat, y_true)\n",
    "        # activation regularisation\n",
    "        for i, (name, activation) in enumerate(sorted(activations.items())):\n",
    "            loss += 0.1*beta*torch.norm(activation, norm)\n",
    "        # bn reg\n",
    "        for j, module in enumerate(model.modules()):\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                loss += 500*beta*torch.norm(module.weight, norm)\n",
    "        # penalize specific layer\n",
    "        loss += 300*beta*torch.norm(model.features[1].bottleneck[0].weight, norm)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Clip parameters\n",
    "#         model = clip_parameters(model)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = MobileNet(n_classes).to(device)\n",
    "#model = nn.DataParallel(model) #torch.cuda.device_count()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, 200, device, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./cifar-convnet.pth\") # don't forget to set model.eval() after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

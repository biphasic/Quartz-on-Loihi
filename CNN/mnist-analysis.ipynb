{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.quantization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import collections\n",
    "from functools import partial\n",
    "from mnist_model import ConvNet\n",
    "import copy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "# parameters\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 10000\n",
    "num_workers = 10\n",
    "\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "valid_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(10)\n",
    "model.load_state_dict(torch.load(\"models/mnist-convnet.pth\", map_location=torch.device(DEVICE)))\n",
    "capture = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mnist_accuracy(model, valid_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = get_weights_biases(model)\n",
    "percentile = 99.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weights = np.array(([np.percentile(weight, percentile) for weight in weights]))\n",
    "min_weights = np.array(([np.percentile(weight, 100-percentile) for weight in weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scaling = [max(ma, abs(mi)) for ma, mi in zip(max_weights, min_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_scaling = [1.347, 1.862, 1.811, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1/(w_scale*a_scale) for w_scale, a_scale in zip(weight_scaling, activation_scaling)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_scaling = np.array(([np.percentile(bias, percentile) for bias in biases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_model = copy.deepcopy(model)\n",
    "scaled_model.eval()\n",
    "with torch.no_grad():\n",
    "    scaled_model.conv1.weight /= weight_scaling[0] * activation_scaling[0]\n",
    "    scaled_model.conv1.bias /= weight_scaling[0] * activation_scaling[0]\n",
    "    scaled_model.conv2.weight /= weight_scaling[1] * activation_scaling[1]\n",
    "    scaled_model.conv2.bias /= weight_scaling[1] * activation_scaling[1]\n",
    "    scaled_model.conv3.weight /= weight_scaling[2] * activation_scaling[2]\n",
    "    scaled_model.conv3.bias /= weight_scaling[2] * activation_scaling[2]\n",
    "    scaled_model.fc1.weight /= weight_scaling[3] * activation_scaling[3]\n",
    "    scaled_model.fc1.bias /= weight_scaling[3] * activation_scaling[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_model\n",
    "get_mnist_accuracy(scaled_model, valid_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(scaled_model, percentile):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    batch_size_test = 1000\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    valid_dataset = datasets.MNIST(root='./data', train=False,transform=transform_test, download=False)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size_test, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    device = 'cuda'\n",
    "    scaled_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scaled_model.eval()\n",
    "\n",
    "        activations = {}\n",
    "        def save_activation(name, mod, inp, out):\n",
    "            if name not in activations.keys():\n",
    "                activations[name] = out\n",
    "            else:\n",
    "                activations[name] = torch.cat((activations[name],out))\n",
    "\n",
    "        names = []\n",
    "        handles = []\n",
    "        max_weights_percentile = []\n",
    "        min_weights_percentile = []\n",
    "\n",
    "        for name, module in scaled_model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                handles.append(module.register_forward_hook(partial(save_activation, name)))\n",
    "                names.append(name)\n",
    "                max_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), percentile).round(2))\n",
    "                min_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), 100-percentile).round(2))\n",
    "\n",
    "        running_loss = 0\n",
    "        for X, y_true in valid_loader:\n",
    "            X = X.to(device)\n",
    "            y_output = scaled_model(X)\n",
    "        [handle.remove() for handle in handles] # remove forward hooks\n",
    "\n",
    "        str_output = ''.join([\"{}: [{},{}]; \".format(names[i], str(min_weights_percentile[i]), str(max_weights_percentile[i])) for i in range(len(names))])\n",
    "        print(\"\\t\" + str(percentile) + \"% weights: \" + str_output)\n",
    "\n",
    "        str_output = ''.join([\"{}: {}, \".format(name, round(np.percentile(np.maximum(activation.cpu(),0), percentile),3)) for name, activation in activations.items()])\n",
    "        print(\"\\t\" + str(percentile) + \"% activations: \" + str_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activations(scaled_model, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activations(model, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

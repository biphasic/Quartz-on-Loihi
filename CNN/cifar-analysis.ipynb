{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.quantization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from cifar_model import MobileNet\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "# parameters\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "valid_dataset = datasets.CIFAR10(root='./data', train=False,transform=transform)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(10)#.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"cifar-convnet.pth\", map_location=torch.device(DEVICE)))\n",
    "capture = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(model, valid_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold bn layers into previous conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_module = None\n",
    "new_layers = []\n",
    "for module in model.modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.MaxPool2d, nn.BatchNorm2d, nn.Linear, nn.ReLU6, nn.ReLU, nn.Flatten)):\n",
    "        if isinstance(module, nn.BatchNorm2d) and isinstance(previous_module, nn.Conv2d):\n",
    "            new_layers[-1] = torch.nn.utils.fuse_conv_bn_eval(previous_module, module)\n",
    "        else:\n",
    "            new_layers.append(module)\n",
    "        previous_module = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model = nn.Sequential(*new_layers)\n",
    "# folded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_folded_accuracy(folded_model, valid_loader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size_test = 1000\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "valid_dataset = datasets.CIFAR10(root='./data', train=False,transform=transform_test, download=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size_test, shuffle=False, num_workers=4, pin_memory=True)\n",
    "device = 'cuda'\n",
    "folded_model.to(device)\n",
    "\n",
    "percentile = 99.9\n",
    "\n",
    "with torch.no_grad():\n",
    "    folded_model.eval()\n",
    "\n",
    "    activations = {}\n",
    "    def save_activation(name, mod, inp, out):\n",
    "        if name not in activations.keys():\n",
    "            activations[name] = out\n",
    "        else:\n",
    "            activations[name] = torch.cat((activations[name],out))\n",
    "\n",
    "    names = []\n",
    "    handles = []\n",
    "    max_weights_percentile = []\n",
    "    min_weights_percentile = []\n",
    "\n",
    "    for name, module in folded_model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            handles.append(module.register_forward_hook(partial(save_activation, name)))\n",
    "            names.append(name)\n",
    "            max_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), percentile).round(2))\n",
    "            min_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), 100-percentile).round(2))\n",
    "\n",
    "    running_loss = 0\n",
    "    for X, y_true in valid_loader:\n",
    "        X = X.to(device)\n",
    "        y_output = folded_model(X)\n",
    "    [handle.remove() for handle in handles] # remove forward hooks\n",
    "\n",
    "    str_output = ''.join([\"{}: [{},{}]; \".format(names[i], str(min_weights_percentile[i]), str(max_weights_percentile[i])) for i in range(len(names))])\n",
    "    print(\"\\t\" + str(percentile) + \"% weights: \" + str_output)\n",
    "\n",
    "    str_output = ''.join([\"{}: {}, \".format(name, round(np.percentile(np.maximum(activation.cpu(),0), percentile),3)) for name, activation in activations.items()])\n",
    "    print(\"\\t\" + str(percentile) + \"% activations: \" + str_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(zip(min_weights_percentile, max_weights_percentile))\n",
    "scaling_factors = [1/max(abs(mini), maxi) for mini, maxi in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "scaled_model = copy.deepcopy(folded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = dict(zip(names[:-1], scaling_factors[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for name, module in scaled_model.named_children():\n",
    "#         if name in my_dict.keys():\n",
    "        if hasattr(module, 'weight'):\n",
    "            module.weight *= 2 # my_dict[name]\n",
    "#             module.bias *= 2 # my_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size_test = 1000\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "valid_dataset = datasets.CIFAR10(root='./data', train=False,transform=transform_test, download=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size_test, shuffle=False, num_workers=4, pin_memory=True)\n",
    "device = 'cuda'\n",
    "scaled_model.to(device)\n",
    "\n",
    "percentile = 99.9\n",
    "\n",
    "with torch.no_grad():\n",
    "    scaled_model.eval()\n",
    "\n",
    "    activations = {}\n",
    "    def save_activation(name, mod, inp, out):\n",
    "        if name not in activations.keys():\n",
    "            activations[name] = out\n",
    "        else:\n",
    "            activations[name] = torch.cat((activations[name],out))\n",
    "\n",
    "    names = []\n",
    "    handles = []\n",
    "    max_weights_percentile = []\n",
    "    min_weights_percentile = []\n",
    "\n",
    "    for name, module in scaled_model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            handles.append(module.register_forward_hook(partial(save_activation, name)))\n",
    "            names.append(name)\n",
    "            max_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), percentile).round(2))\n",
    "            min_weights_percentile.append(np.percentile(module.weight.cpu().numpy(), 100-percentile).round(2))\n",
    "\n",
    "    running_loss = 0\n",
    "    for X, y_true in valid_loader:\n",
    "        X = X.to(device)\n",
    "        y_output = scaled_model(X)\n",
    "    [handle.remove() for handle in handles] # remove forward hooks\n",
    "\n",
    "    str_output = ''.join([\"{}: [{},{}]; \".format(names[i], str(min_weights_percentile[i]), str(max_weights_percentile[i])) for i in range(len(names))])\n",
    "    print(\"\\t\" + str(percentile) + \"% weights: \" + str_output)\n",
    "\n",
    "    str_output = ''.join([\"{}: {}, \".format(name, round(np.percentile(np.maximum(activation.cpu(),0), percentile),3)) for name, activation in activations.items()])\n",
    "    print(\"\\t\" + str(percentile) + \"% activations: \" + str_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_folded_accuracy(scaled_model, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integer(weights, biases, bitwidth, normalize=True):\n",
    "    \"\"\"Convert weights and biases to integers.\n",
    "    :param np.ndarray weights: 2D or 4D weight tensor.\n",
    "    :param np.ndarray biases: 1D bias vector.\n",
    "    :param int bitwidth: Number of bits for integer conversion.\n",
    "    :param bool normalize: Whether to normalize weights and biases by the\n",
    "        common maximum before quantizing.\n",
    "    :return: The quantized weights and biases.\n",
    "    :rtype: tuple[np.ndarray, np.ndarray]\n",
    "    \"\"\"\n",
    "\n",
    "    max_val = np.max(np.abs(np.concatenate([weights, biases], None))) if normalize else 1\n",
    "    a_min = -2**bitwidth\n",
    "    a_max = - a_min - 1\n",
    "    weights = np.clip(weights / max_val * a_max, a_min, a_max).astype(int)\n",
    "    biases = np.clip(biases / max_val * a_max, a_min, a_max).astype(int)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

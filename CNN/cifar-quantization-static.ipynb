{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from cifar_model import MobileNet, ConvBNReLU, Bottleneck, ConvPool\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import ipdb\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static quantization of our CIFAR model\n",
    "# adapted from https://github.com/leimao/PyTorch-Static-Quantization/blob/main/cifar.py\n",
    "# Lei has great tutorials on quantization aware training too!\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "\n",
    "def prepare_dataloader(num_workers=8,\n",
    "                       train_batch_size=128,\n",
    "                       eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=train_transform)\n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                               batch_size=train_batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                              batch_size=eval_batch_size,\n",
    "                                              sampler=test_sampler,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                train_loader,\n",
    "                test_loader,\n",
    "                device,\n",
    "                learning_rate=1e-1,\n",
    "                num_epochs=200):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=learning_rate,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=[100, 150],\n",
    "                                                     gamma=0.1,\n",
    "                                                     last_epoch=-1)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = evaluate_model(model=model,\n",
    "                                              test_loader=test_loader,\n",
    "                                              device=device,\n",
    "                                              criterion=criterion)\n",
    "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n",
    "        -1, eval_loss, eval_accuracy))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model,\n",
    "                                                  test_loader=test_loader,\n",
    "                                                  device=device,\n",
    "                                                  criterion=criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n",
    "            .format(epoch, train_loss, train_accuracy, eval_loss,\n",
    "                    eval_accuracy))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class QuantizedMobileNet(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedMobileNet, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def model_equivalence(model_1,\n",
    "                      model_2,\n",
    "                      device,\n",
    "                      rtol=1e-05,\n",
    "                      atol=1e-08,\n",
    "                      num_tests=100,\n",
    "                      input_size=(1, 3, 32, 32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol,\n",
    "                       equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "num_classes = 10\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "\n",
    "model_dir = \"models\"\n",
    "model_filename = \"cifar-convnet.pth\"\n",
    "quantized_model_filename = \"cifar-convnet-quantized.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "quantized_model_filepath = os.path.join(model_dir,\n",
    "                                        quantized_model_filename)\n",
    "\n",
    "model = MobileNet(10)\n",
    "model.load_state_dict(torch.load(\"models/cifar-convnet.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "set_random_seeds(random_seed=random_seed)\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=8,\n",
    "                                               train_batch_size=128,\n",
    "                                               eval_batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "# Move the model to CPU since static quantization does not support CUDA currently.\n",
    "model.to(cpu_device)\n",
    "# Make a copy of the model for layer fusion\n",
    "fused_model = copy.deepcopy(model)\n",
    "\n",
    "model.eval()\n",
    "# The model has to be switched to evaluation mode before any layer fusion.\n",
    "# Otherwise the quantization will not work correctly.\n",
    "capture = fused_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e424b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in fused_model.modules():\n",
    "    if type(m) == ConvBNReLU:\n",
    "        torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "    if type(m) == Bottleneck:\n",
    "        for name, block in m.named_children():\n",
    "            if name == 'bottleneck':\n",
    "                torch.quantization.fuse_modules(block, ['0', '1', '2'], inplace=True)\n",
    "                torch.quantization.fuse_modules(block, ['4', '5',], inplace=True)\n",
    "    if type(m) == ConvPool:\n",
    "        torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fused model.\n",
    "# print(fused_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and fused model should be equivalent.\n",
    "assert model_equivalence(\n",
    "    model_1=model,\n",
    "    model_2=fused_model,\n",
    "    device=cpu_device,\n",
    "    rtol=1e-03,\n",
    "    atol=1e-06,\n",
    "    num_tests=100,\n",
    "    input_size=(\n",
    "        1, 3, 32,\n",
    "        32)), \"Fused model is not equivalent to the original model!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861456d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "quantized_model = QuantizedMobileNet(model_fp32=fused_model)\n",
    "# Using un-fused model will fail.\n",
    "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
    "# quantized_model = QuantizedResNet18(model_fp32=model)\n",
    "# Select quantization schemes from\n",
    "# https://pytorch.org/docs/stable/quantization-support.html\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "# Custom quantization configurations\n",
    "# quantization_config = torch.quantization.default_qconfig\n",
    "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
    "\n",
    "quantized_model.qconfig = quantization_config\n",
    "\n",
    "# Print quantization configurations\n",
    "print(quantized_model.qconfig)\n",
    "\n",
    "# https://pytorch.org/docs/master/torch.quantization.html#torch.quantization.prepare\n",
    "torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# Use training data for calibration.\n",
    "calibrate_model(model=quantized_model,\n",
    "                loader=train_loader,\n",
    "                device=cpu_device)\n",
    "\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using high-level static quantization wrapper\n",
    "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "# quantized_model = torch.quantization.quantize(model=quantized_model, run_fn=calibrate_model, run_args=[train_loader], mapping=None, inplace=False)\n",
    "\n",
    "compressed_model.eval()\n",
    "\n",
    "# Print quantized model.\n",
    "# print(quantized_model)\n",
    "\n",
    "# Save quantized model.\n",
    "save_torchscript_model(model=compressed_model,\n",
    "                       model_dir=model_dir,\n",
    "                       model_filename=quantized_model_filename)\n",
    "\n",
    "# Load quantized model.\n",
    "quantized_jit_model = load_torchscript_model(\n",
    "    model_filepath=quantized_model_filepath, device=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f86a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fp32_eval_accuracy = evaluate_model(model=model,\n",
    "                                       test_loader=test_loader,\n",
    "                                       device=cpu_device,\n",
    "                                       criterion=None)\n",
    "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model,\n",
    "                                       test_loader=test_loader,\n",
    "                                       device=cpu_device,\n",
    "                                       criterion=None)\n",
    "\n",
    "# Skip this assertion since the values might deviate a lot.\n",
    "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=model,\n",
    "                                                       device=cpu_device,\n",
    "                                                       input_size=(1, 3,\n",
    "                                                                   32, 32),\n",
    "                                                       num_samples=100)\n",
    "int8_cpu_inference_latency = measure_inference_latency(\n",
    "    model=quantized_model,\n",
    "    device=cpu_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "int8_jit_cpu_inference_latency = measure_inference_latency(\n",
    "    model=quantized_jit_model,\n",
    "    device=cpu_device,\n",
    "    input_size=(1, 3, 32, 32),\n",
    "    num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=model,\n",
    "                                                       device=cuda_device,\n",
    "                                                       input_size=(1, 3,\n",
    "                                                                   32, 32),\n",
    "                                                       num_samples=100)\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(\n",
    "    fp32_cpu_inference_latency * 1000))\n",
    "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(\n",
    "    fp32_gpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(\n",
    "    int8_cpu_inference_latency * 1000))\n",
    "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(\n",
    "    int8_jit_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f18df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.eval()\n",
    "# type(quantized_model.model_fp32.features[1].bottleneck[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layers = []\n",
    "for module in quantized_model.modules():\n",
    "    if isinstance(module, (torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d, torch.nn.quantized.modules.conv.Conv2d, torch.nn.quantized.modules.linear.Linear, nn.ReLU6, nn.MaxPool2d, nn.ReLU, nn.Flatten)):\n",
    "        new_layers.append(module)\n",
    "compressed_model = nn.Sequential(*new_layers)\n",
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e490b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layers = []\n",
    "for module in quantized_model.modules():\n",
    "    if isinstance(module, (torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d, torch.nn.quantized.modules.conv.Conv2d)):\n",
    "        new_module = nn.Conv2d(module.in_channels, module.out_channels, module.kernel_size, stride=module.stride, padding=module.padding, groups=module.groups)\n",
    "        new_module.weight = torch.nn.Parameter(module.weight().int_repr().float()/128, requires_grad=False)\n",
    "#         ipdb.set_trace()\n",
    "        new_module.bias = torch.nn.Parameter(module.bias(), requires_grad=False)\n",
    "        new_layers.append(new_module)\n",
    "        if isinstance(module, torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d):\n",
    "            new_layers.append(nn.ReLU())\n",
    "    elif isinstance(module, torch.nn.quantized.modules.linear.Linear):\n",
    "        new_module = nn.Linear(module.in_features, module.out_features)\n",
    "        new_module.weight = torch.nn.Parameter(module.weight().int_repr().float()/128, requires_grad=False)\n",
    "        new_module.bias = torch.nn.Parameter(module.bias(), requires_grad=False)\n",
    "        new_layers.append(new_module)\n",
    "    elif isinstance(module, (nn.ReLU6, nn.MaxPool2d, nn.ReLU, nn.Flatten)):\n",
    "        new_layers.append(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model = nn.Sequential(*new_layers)\n",
    "# folded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(folded_model, \"./cifar-convnet-quantized2.pth\") # don't forget to set model.eval() after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model = torch.load(\"cifar-convnet-quantized.pth\")\n",
    "folded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefe358",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "folded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf38216",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "valid_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=False, num_workers=10, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model.to(device)\n",
    "get_folded_accuracy(folded_model, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, target = next(iter(valid_loader))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_inputs = torch.tensor((inputs*255).clone().detach().requires_grad_(False), dtype=torch.int8)\n",
    "# new_inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5717095",
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model(inputs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ce851",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model,\n",
    "                                       test_loader=test_loader,\n",
    "                                       device=cpu_device,\n",
    "                                       criterion=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33397577",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b33f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_jit_model.model_fp32.features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification on Loihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN.mnist_model import ConvNet\n",
    "from CNN.utils import get_weights_biases\n",
    "import torch\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "import quartz\n",
    "from quartz import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained ANN model and inspect parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "model = ConvNet(n_classes)\n",
    "model.load_state_dict(torch.load(\"CNN/models/mnist-convnet.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = get_weights_biases(model)\n",
    "print(([weight.max() for weight in weights]))\n",
    "print(([weight.min() for weight in weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last layer activity will necessarily be a bit higher because of logits, so need to scale down params here\n",
    "weights[3] /= 3\n",
    "biases[3] /= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build Quartz model with parameters from ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**7\n",
    "input_dims = (1,28,28)\n",
    "pool_kernel_size = [2,2]\n",
    "batch_size = 100\n",
    "\n",
    "loihi_model = quartz.Network(t_max, verbose=True, layers=[\n",
    "    layers.InputLayer(dims=input_dims),\n",
    "    layers.Conv2D(weights=weights[0], biases=biases[0]),\n",
    "    layers.MaxPool2D(kernel_size=pool_kernel_size),\n",
    "    layers.Conv2D(weights=weights[1], biases=biases[1]),\n",
    "    layers.MaxPool2D(kernel_size=pool_kernel_size),\n",
    "    layers.Conv2D(weights=weights[2], biases=biases[2]),\n",
    "    layers.Dense(weights=weights[3], biases=biases[3]), \n",
    "])\n",
    "\n",
    "n_cores_per_layer = [0,4,1,2,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loihi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data and classify or run the power benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),])\n",
    "test_set = datasets.MNIST('./CNN/data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to skip to a certain batch in case script failed midway\n",
    "# for i in range(6):\n",
    "#     inputs, target = next(test_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"Started on %a, %d %b %Y %H:%M:%S\", time.gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "profiling = False # change this flag for classification or power benchmarks!\n",
    "\n",
    "errors = []\n",
    "avg_first_spikes = []\n",
    "for b, (inputs, target) in enumerate(test_loader_iter):\n",
    "    if not profiling:\n",
    "        loihi_output_values = loihi_model(inputs.detach().numpy(), n_cores_per_layer, partition='loihi_2h', logging=True, profiling=profiling)\n",
    "        # some of the outputs might spike multiple times so instead of the output values, we rely on the first spikes for every batch\n",
    "        avg_first_spikes.append(np.mean([time-i*loihi_model.steps_per_image for i, time in enumerate(np.min(loihi_model.first_spikes, axis=0))]))\n",
    "        print(\"Average first spike: \" + str(avg_first_spikes[-1]))\n",
    "        classification_results = np.argmin(loihi_model.first_spikes, axis=0)\n",
    "        positives = sum(classification_results == target.numpy())\n",
    "        errors.append(100*(1-positives/len(target)))\n",
    "        print(\"Correctly detected {} out of {}: {}% error\".format(positives, len(target), str(errors[-1])))\n",
    "    else:\n",
    "        energy_probe = loihi_model(inputs.detach().numpy(), partition='nahuku32_2h', logging=True, profiling=profiling)\n",
    "        break\n",
    "    \n",
    "    print(\"Batch {} finished within {} seconds.\".format(b, time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loihi_model.compartments_on_core.reshape(-1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loihi_model.compartments_on_core[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(avg_first_spikes)/len(avg_first_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(errors)/len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist-results.txt\", \"a\") as myfile:\n",
    "    myfile.write(\"{}\\n\".format(errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nxsdk-1.0.0",
   "language": "python",
   "name": "nxsdk-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
